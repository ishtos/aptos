{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations import torch as AT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # settings\n",
    "    'seed': 43,\n",
    "    'num_workers': cpu_count(),\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # data\n",
    "    'data_path': os.path.join('..', 'input'),\n",
    "    \n",
    "    # optimizer\n",
    "    'optimizer_name': 'Adam',\n",
    "    'lr': 0.0001,\n",
    "    \n",
    "    # model\n",
    "    'model_name': 'resnet34',\n",
    "    'pretrained': True,\n",
    "    'weight_path': None,\n",
    "    \n",
    "    # loss\n",
    "    'loss_name': 'CrossEntropy',\n",
    "    \n",
    "    # transforms\n",
    "#     'transforms': 'pytorch',\n",
    "    'transforms': 'albumentations',\n",
    "    \n",
    "    'pytorch': {\n",
    "        'resize': {'train': False, 'test': False, 'train_size': 256, 'test_size': 224},\n",
    "        'centerCrop': {'train': True, 'test': False, 'train_size': 224,'test_size': 224},\n",
    "        'randomHorizontalFlip': {'train': True, 'test': False},\n",
    "        'randomRotation': {'train': True, 'test': True, 'degrees': 180},\n",
    "        'toTensor': {'train': True, 'test': True},\n",
    "        'normalize': {'train': True, 'test': True},\n",
    "    },\n",
    "    \n",
    "    'albumentations': {\n",
    "        'resize': {'train': False, 'test': False, 'train_size': 256, 'test_size': 224},\n",
    "        'centerCrop': {'train': True, 'test': False, 'train_size': 224,'test_size': 224},\n",
    "        'horizontalFlip': {'train': True, 'test': False},\n",
    "        'rotate': {'train': True, 'test': False, 'limit': 180},\n",
    "        'clahe': {'train': True, 'test': True},\n",
    "        'gaussNoise': {'train': True, 'test': True},\n",
    "        'randomBrightness': {'train': True, 'test': True},\n",
    "        'randomContrast': {'train': True, 'test': True},\n",
    "        'randomBrightnrssContrast': {'train': False, 'test': False},\n",
    "        'hueSaturationValue': {'train': True, 'test': True},\n",
    "        'toTensor': {'train': True, 'test': True},\n",
    "        'normalize': {'train': True, 'test': True},\n",
    "    },\n",
    "    \n",
    "    # train settings\n",
    "    'image_size': 256,\n",
    "    'epochs': 100,\n",
    "    'patience': 10,\n",
    "    'verbose': True,\n",
    "    'batch_size': 32,\n",
    "    'valid_size': 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything():\n",
    "    seed = config['seed']\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_from_gray(image, tol=7):\n",
    "    if image.ndim == 2:\n",
    "        mask = image > tol\n",
    "        return image[np.ix_(mask.any(1), mask.any(0))]\n",
    "   \n",
    "    elif image.ndim == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_image > tol\n",
    "        \n",
    "        check_shape = image[:,:,0][np.ix_(mask.any(1), mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): \n",
    "            return image\n",
    "        \n",
    "        else:\n",
    "            imageR = image[:,:,0][np.ix_(mask.any(1), mask.any(0))]\n",
    "            imageG = image[:,:,1][np.ix_(mask.any(1), mask.any(0))]\n",
    "            imageB = image[:,:,2][np.ix_(mask.any(1), mask.any(0))]\n",
    "            image = np.stack([imageR, imageG, imageB], axis=-1)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "def preprocess(image_name):\n",
    "    image = cv2.imread(image_name)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = crop_image_from_gray(image)\n",
    "    image = cv2.resize(image, (config['image_size'], config['image_size']))\n",
    "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), 30), -4, 128)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model_name = config['model_name']\n",
    "    pretrained = config['pretrained']\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(pretrained=pretrained)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = models.resnet34(pretrained=pretrained)\n",
    "    elif model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=pretrained)\n",
    "    elif model_name == 'resnet101':\n",
    "        model = models.resnet101(pretrained=pretrained)\n",
    "    elif model_name == 'resnet152':\n",
    "        model = models.resnet152(pretrained=pretrained)\n",
    "    elif model_name == 'vgg11_bn':\n",
    "        model = models.vgg11_bn(pretrained=pretrained)\n",
    "    elif model_name == 'vgg13_bn':\n",
    "        model = models.vgg13_bn(pretrained=pretrained)\n",
    "    elif model_name == 'vgg16_bn':\n",
    "        model = models.vgg16_bn(pretrained=pretrained)\n",
    "    elif model_name == 'vgg19_bn':\n",
    "        model = models.vgg19_bn(pretrained=pretrained)\n",
    "    elif model_name == 'inception_v3':\n",
    "        model = models.inception_v3(pretrained=pretrain)\n",
    "        \n",
    "    return model\n",
    "\n",
    "def get_optimizer(params): \n",
    "    optimizer_name = config['optimizer_name']\n",
    "    lr = config['lr']\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(params=params, lr=lr)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(params=params, lr=lr)\n",
    "    elif optimizer_name == 'Adagrad':\n",
    "        optimizer = optim.Adagrad(params=params, lr=lr)\n",
    "    elif optimizer_name == 'Adadelta':\n",
    "        optimizer = optim.Adadelta(params=params, lr=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(params=params, lr=lr)\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def get_loss():\n",
    "    loss_name = config['loss_name']\n",
    "    if loss_name == 'MSE':\n",
    "        loss = nn.MSELoss()\n",
    "    elif loss_name == 'CrossEntropy':\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "    elif loss_name == 'BCE':\n",
    "        loss = nn.BCELoss()\n",
    "    elif loss_name == 'BCEWithLogits':\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def get_label_data(train):\n",
    "    \n",
    "    return train['diagnosis'].values\n",
    "                                \n",
    "def get_transforms_train():\n",
    "    transforms_train_list = []\n",
    "\n",
    "    if config['transforms'] == 'pytorch':\n",
    "        if config['pytorch']['resize']['train']:\n",
    "            transforms_train_list.append(transforms.Resize(size=(config['pytorch']['resize']['train_size'], config['pytorch']['resize']['train_size'])))\n",
    "        if config['pytorch']['centerCrop']['train']:\n",
    "            transforms_train_list.append(transforms.Resize(size=(config['pytorch']['centerCrop']['train_size'], config['pytorch']['centerCrop']['train_size'])))\n",
    "        if config['pytorch']['randomHorizontalFlip']['train']:\n",
    "            transforms_train_list.append(transforms.RandomHorizontalFlip())\n",
    "        if config['pytorch']['randomRotation']['train']:\n",
    "            transforms_train_list.append(transforms.RandomRotation(degrees=config['pytorch']['randomRotation']['degrees']))\n",
    "        if config['pytorch']['toTensor']['train']:\n",
    "            transforms_train_list.append(transforms.ToTensor())\n",
    "        if config['pytorch']['normalize']['train']:\n",
    "            transforms_train_list.append(transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "        train_transforms = transforms.Compose(transforms_train_list)\n",
    "    else:\n",
    "        if config['albumentations']['resize']['train']:\n",
    "            transforms_train_list.append(A.Resize(config['albumentations']['resize']['train_size'], config['albumentations']['resize']['train_size']))\n",
    "        if config['pytorch']['centerCrop']['train']:\n",
    "            transforms_train_list.append(transforms.Resize(config['pytorch']['centerCrop']['train_size'], config['pytorch']['centerCrop']['train_size']))\n",
    "        if config['albumentations']['horizontalFlip']['train']:\n",
    "            transforms_train_list.append(A.HorizontalFlip())\n",
    "        if config['albumentations']['rotate']['train']:\n",
    "            transforms_train_list.append(A.Rotate(config['albumentations']['rotate']['limit']))\n",
    "        if config['albumentations']['clahe']['train']:\n",
    "            transforms_train_list.append(A.CLAHE())\n",
    "        if config['albumentations']['gaussNoise']['train']:\n",
    "            transforms_train_list.append(A.GaussNoise())\n",
    "        if config['albumentations']['randomBrightness']['train']:\n",
    "            transforms_train_list.append(A.RandomBrightness())\n",
    "        if config['albumentations']['randomContrast']['train']:\n",
    "            transforms_train_list.append(A.RandomContrast())\n",
    "        if config['albumentations']['randomBrightnrssContrast']['train']:\n",
    "            transforms_train_list.append(A.RandomBrightnessContrast())\n",
    "        if config['albumentations']['hueSaturationValue']['train']:\n",
    "            transforms_train_list.append(A.HueSaturationValue())\n",
    "        if config['albumentations']['normalize']['train']:\n",
    "            transforms_train_list.append(A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "        if config['albumentations']['toTensor']['train']:\n",
    "            transforms_train_list.append(AT.ToTensor())\n",
    "        train_transforms = A.Compose(transforms_train_list)\n",
    "        \n",
    "    return train_transforms\n",
    "                                \n",
    "def get_transforms_test():\n",
    "    transforms_test_list = []\n",
    "\n",
    "    if config['transforms'] == 'pytorch':\n",
    "        if config['pytorch']['resize']['test']:\n",
    "            transforms_test_list.append(transforms.Resize(size=(config['pytorch']['resize']['test_size'], config['pytorch']['resize']['test_size'])))\n",
    "        if config['pytorch']['centerCrop']['test']:\n",
    "            transforms_test_list.append(transforms.Resize(size=(config['pytorch']['centerCrop']['test_size'], config['pytorch']['centerCrop']['test_size'])))\n",
    "        if config['pytorch']['randomHorizontalFlip']['test']:\n",
    "            transforms_test_list.append(transforms.RandomHorizontalFlip())\n",
    "        if config['pytorch']['randomRotation']['test']:\n",
    "            transforms_test_list.append(transforms.RandomRotation(degrees=config['pytorch']['randomRotation']['degrees']))\n",
    "        if config['pytorch']['toTensor']['test']:\n",
    "            transforms_test_list.append(transforms.ToTensor())\n",
    "        if config['pytorch']['normalize']['test']:\n",
    "            transforms_test_list.append(transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "        test_transforms = transforms.Compose(transforms_test_list)\n",
    "    else:\n",
    "        if config['albumentations']['resize']['test']:\n",
    "            transforms_test_list.append(A.Resize(config['albumentations']['resize']['test_size'], config['albumentations']['resize']['test_size']))\n",
    "        if config['pytorch']['centerCrop']['test']:\n",
    "            transforms_test_list.append(transforms.Resize(config['albumentations']['centerCrop']['test_size'], config['albumentations']['centerCrop']['test_size']))\n",
    "        if config['albumentations']['horizontalFlip']['test']:\n",
    "            transforms_test_list.append(A.HorizontalFlip())\n",
    "        if config['albumentations']['rotate']['test']:\n",
    "            transforms_test_list.append(A.Rotate(config['albumentations']['rotate']['limit']))\n",
    "        if config['albumentations']['clahe']['test']:\n",
    "            transforms_test_list.append(A.CLAHE())\n",
    "        if config['albumentations']['gaussNoise']['test']:\n",
    "            transforms_test_list.append(A.GaussNoise())\n",
    "        if config['albumentations']['randomBrightness']['test']:\n",
    "            transforms_test_list.append(A.RandomBrightness())\n",
    "        if config['albumentations']['randomContrast']['test']:\n",
    "            transforms_test_list.append(A.RandomContrast())\n",
    "        if config['albumentations']['randomBrightnrssContrast']['test']:\n",
    "            transforms_test_list.append(A.RandomBrightnessContrast())\n",
    "        if config['albumentations']['hueSaturationValue']['test']:\n",
    "            transforms_test_list.append(A.HueSaturationValue())\n",
    "        if config['albumentations']['normalize']['test']:\n",
    "            transforms_test_list.append(A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "        if config['albumentations']['toTensor']['test']:\n",
    "            transforms_test_list.append(AT.ToTensor())\n",
    "        test_transforms = A.Compose(transforms_test_list)\n",
    "        \n",
    "    return test_transforms                       \n",
    "        \n",
    "def get_train_data():\n",
    "    train = pd.read_csv(os.path.join(config['data_path'], 'train.csv'))\n",
    "    y = get_label_data(train)\n",
    "    \n",
    "    train_dataset = TrainDataset(id_code=train['id_code'].values, transform=get_transforms_train(), y=y)\n",
    "    print(len(train_dataset)) # debug\n",
    "    tr, val = train_test_split(train['diagnosis'], stratify=train['diagnosis'], test_size=config['valid_size'])\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(list(tr.index))\n",
    "    valid_sampler = SubsetRandomSampler(list(val.index))\n",
    "                                \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], sampler=train_sampler, num_workers=config['num_workers'])\n",
    "    valid_loader = DataLoader(train_dataset, batch_size=config['batch_size'], sampler=valid_sampler, num_workers=config['num_workers'])\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "    \n",
    "def get_test_data():\n",
    "    test = pd.read_csv(os.path.join(config['data_path'], 'test.csv'))\n",
    "\n",
    "    test_dataset = TestDataset(id_code=test['id_code'].values, transform=get_transforms_test())\n",
    "    print(len(test_dataset)) # debug\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], num_workers=config['num_workers'])\n",
    "    \n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & valid & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    for _, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(config['device']), target.to(config['device'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.float(), target)\n",
    "        running_loss += loss.data\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def valid(model, criterion, optimizer, valid_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0\n",
    "    for _, (data, target) in enumerate(valid_loader):\n",
    "        data, target = data.to(config['device']), target.to(config['device'])\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output.float(), target)\n",
    "        running_loss += loss.data\n",
    "        \n",
    "    # TODO: implementation\n",
    "#         output = output.cpu().detach().numpy()\n",
    "#         target = target.cpu().detach().numpy()\n",
    "#         res = np.zeros(output.shape[0])\n",
    "#         for i, e in enumerate(output):\n",
    "#             res[i] = np.argmax(e)\n",
    "        \n",
    "#         score = 1 - cohen_kappa_score(res, target)\n",
    "#         print(score)\n",
    "        \n",
    "    return running_loss / len(valid_loader)\n",
    "\n",
    "def test(model, test_loader, sub):\n",
    "    model.eval()\n",
    "    \n",
    "    for (data, _, name) in test_loader:\n",
    "        data = data.to(config['device'])\n",
    "            \n",
    "        output = model(data)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        \n",
    "        for i, (e, n) in enumerate(list(zip(output, name))):\n",
    "#             sub.loc[sub['id_code'] == n.split('/')[-1].split('.')[0], 'diagnosis'] = le.inverse_transform([np.argmax(e)])\n",
    "            sub.loc[sub['id_code'] == n.split('/')[-1].split('.')[0], 'diagnosis'] = np.argmax(e)\n",
    "    \n",
    "    return sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(DRModel, self).__init__()\n",
    "        self.model = get_model()\n",
    "        if config['weight_path'] != None:\n",
    "            self.model.load_state_dict(torch.load(config['weight_path']))\n",
    "        self.model.fc = nn.Linear(in_features=self.model.fc.in_features, out_features=5, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "        x = F.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, id_code, transform, y):\n",
    "        \n",
    "        self.image_name_list = [os.path.join(config['data_path'], 'train_images', f'{image_name}.png') for image_name in tqdm(id_code)]\n",
    "        self.image_list = [preprocess(image_name) for image_name in tqdm(self.image_name_list)]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.labels = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.image_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_name = self.image_name_list[idx]\n",
    "        image = self.image_list[idx]\n",
    "        \n",
    "        if config['transforms'] == 'pytorch':\n",
    "            image = transforms.ToPILImage()(image)\n",
    "            image = self.transform(image)\n",
    "        elif config['transforms'] == 'albumentations':\n",
    "            image = self.transform(image)\n",
    "            image = image['image']\n",
    "            \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, id_code, transform):\n",
    "        \n",
    "        self.image_name_list = [os.path.join(config['data_path'], 'test_images', f'{i}.png') for i in tqdm(id_code)]\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.labels = np.zeros((len(self.image_name_list), 5))\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.image_name_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_name = self.image_name_list[idx]\n",
    "        image = preprocess(image_name)\n",
    "        \n",
    "        if config['transforms'] == 'pytorch':\n",
    "            image = transforms.ToPILImage()(image)\n",
    "            image = self.transform(image)\n",
    "        elif config['transforms'] == 'albumentations':\n",
    "            image = self.transform(image=image)\n",
    "            image = image['image']\n",
    "            \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label, image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self):\n",
    " \n",
    "        self.patience = config['patience']\n",
    "        self.verbose = config['verbose']\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'best.pth')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DRModel().to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3662/3662 [00:00<00:00, 413658.16it/s]\n",
      "100%|██████████| 3662/3662 [27:24<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = get_loss()\n",
    "optimizer = get_optimizer(params=model.parameters())\n",
    "early_stopping = EarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f78f0ad4630>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-8-1e49aa23ba9f>\", line 24, in __getitem__\n    image = self.transform(image)\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/albumentations/core/composition.py\", line 164, in __call__\n    need_to_run = force_apply or random.random() < self.p\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d1cbcda609b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-8-1e49aa23ba9f>\", line 24, in __getitem__\n    image = self.transform(image)\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/albumentations/core/composition.py\", line 164, in __call__\n    need_to_run = force_apply or random.random() < self.p\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    }
   ],
   "source": [
    "for k, (i, j) in enumerate(train_loader):\n",
    "    print(i, j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-8-1e49aa23ba9f>\", line 24, in __getitem__\n    image = self.transform(image)\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/albumentations/core/composition.py\", line 164, in __call__\n    need_to_run = force_apply or random.random() < self.p\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4298f68d1e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {:d}, loss: {:.4f} val_loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2ae064c09f41>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-8-1e49aa23ba9f>\", line 24, in __getitem__\n    image = self.transform(image)\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/albumentations/core/composition.py\", line 164, in __call__\n    need_to_run = force_apply or random.random() < self.p\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(config['epochs'])):\n",
    "    train_loss = train(model, criterion, optimizer, train_loader)\n",
    "    val_loss = valid(model, criterion, optimizer, valid_loader)\n",
    "\n",
    "    print('epoch {:d}, loss: {:.4f} val_loss: {:.4f}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_loader, valid_loader\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = get_test_data()\n",
    "\n",
    "# sub = pd.read_csv(os.path.join(config['data_path'], 'sample_submission.csv'))\n",
    "# sub = test(model, test_loader, sub)\n",
    "# sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEB_HOOK_URL = 'https://hooks.slack.com/services/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'\n",
    "requests.post(WEB_HOOK_URL, data = json.dumps({\n",
    "    'text': f'{config}\\nbest score: {early_stopping.best_score}',  \n",
    "    'username': 'Kaggle Notification',  \n",
    "    'icon_emoji': ':kerneler:',  \n",
    "    'link_names': 1,  \n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
